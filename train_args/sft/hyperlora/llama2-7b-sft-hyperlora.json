{
    "output_dir": "output/llama2-7b-sft-hyperlora",
    "model_name_or_path": "/mnt/liunayu/pretrained_models/meta-llama/Llama-2-7b-hf/",
    "train_file": "data/role_bench_en_train_without_instruction.json",
    "role_desc_file": "./data/roles_desc_en.json",
    "template_name": "llama2",
    "train_mode": "hyperlora",
    "num_train_epochs": 3,
    "per_device_train_batch_size": 8,
    "gradient_accumulation_steps": 1,
    "learning_rate": 5e-5,
    "max_seq_length": 1024,
    "logging_steps": 10,
    "save_steps": 5000,
    "lr_scheduler_type": "cosine",
    "warmup_ratio": 0.1,
    "gradient_checkpointing": true,
    
    "seed": 42,
    "bf16": true,
    "save_strategy": "steps",
    "remove_unused_columns": false,

    "roles_num": 5,
    "roles_emb_dim": 256,
    "layers_num": 224,
    "layers_emb_dim": 128,
    "residual_blocks_num": 2,
    "hyper_hidden_dim": 728,
    "rank_dim": 8,
    "alpha": 32,
    "layernorm_input": true,
    "layernorm_output": true,
    "dropout": 0.0
}
