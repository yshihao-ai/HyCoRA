{
    "output_dir": "output/qwen2.5-7b-sft-hyperlora",
    "model_name_or_path": "/mnt/yangshihao/pretrained_models/qwen2.5-7b/",
    "train_file": "./data/role_bench_zh_train_without_instruction.json",
    "role_desc_file": "./data/roles_desc_zh.json",
    "template_name": "default",
    "train_mode": "hyperlora",
    "num_train_epochs": 5,
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 1,
    "learning_rate": 5e-6,
    "max_seq_length": 1024,
    "logging_steps": 10,
    "save_steps": 1000,
    "lr_scheduler_type": "cosine",
    "warmup_ratio": 0.1,
    
    "seed": 42,
    "bf16": true,
    "save_strategy": "steps",
    "remove_unused_columns": false,

    "roles_num": 5,
    "roles_emb_dim": 128,
    "layers_num": 196,
    "layers_emb_dim": 128,
    "residual_blocks_num": 2,
    "hyper_hidden_dim": 512,
    "rank_dim": 8,
    "alpha": 32,
    "layernorm_input": true,
    "layernorm_output": true,
    "dropout": 0.0
}
